{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iisuslik43/dul/blob/hw2/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J3Uhb_XoIlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import pickle\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIcxtmMyoLQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_losses(train_losses, test_losses):\n",
        "    plt.plot(list(range(len(train_losses))), train_losses, color='r', label='train')\n",
        "    plt.plot(list(range(len(test_losses))), test_losses, color='b', label='test')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "def sample_data():\n",
        "  count = 100000\n",
        "  rand = np.random.RandomState(0)\n",
        "  a = [[-1.5, 2.5]] + rand.randn(count // 3, 2) * 0.2\n",
        "  b = [[1.5, 2.5]] + rand.randn(count // 3, 2) * 0.2\n",
        "  c = np.c_[2 * np.cos(np.linspace(0, np.pi, count // 3)),\n",
        "  -np.sin(np.linspace(0, np.pi, count // 3))]\n",
        "\n",
        "  c += rand.randn(*c.shape) * 0.2\n",
        "  data_x = np.concatenate([a, b, c], axis=0)\n",
        "  data_y = np.array([0] * len(a) + [1] * len(b) + [2] * len(c))\n",
        "  perm = rand.permutation(len(data_x))\n",
        "  return data_x[perm], data_y[perm]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI20PoCCCXOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K = 10\n",
        "X, y = sample_data()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLejoqiGRYbI",
        "colab_type": "code",
        "outputId": "c103e693-ac20-479a-f703-5d18cfefd1b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "image = np.zeros((800, 800))\n",
        "for (x1, x2), y1 in zip(X, y):\n",
        "  image[int(x1 * 100 + 400)][int(x2 * 100 + 400)] = y1 + 1\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33u4GcKNHuvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaskedLinear(nn.Linear):\n",
        "    def __init__(self, in_features, out_features, ms, bias=True):\n",
        "        super().__init__(in_features, out_features, bias)\n",
        "        m1, m2 = ms\n",
        "        mask = m2.reshape(-1, 1) >= m1.reshape(1, -1)\n",
        "        \n",
        "        mask = mask.astype(np.float32)\n",
        "        self.register_buffer('mask', torch.tensor(mask))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return F.linear(x, self.mask * self.weight, self.bias)\n",
        "\n",
        "class MADE(nn.Module):\n",
        "    def __init__(self, d=2, n=200, hidden_sizes=[200, 200]):\n",
        "        super(MADE, self).__init__()\n",
        "        self.d = d\n",
        "        self.n = n\n",
        "        input_size = self.d * self.n\n",
        "        m = [np.array([1, 2])]\n",
        "        last_m = np.repeat(np.arange(self.d), self.n) + 1\n",
        "        for layer_size in hidden_sizes:\n",
        "            m += [np.random.randint(m[-1].min(), d, size=layer_size)]\n",
        "        \n",
        "        layers = [\n",
        "            MaskedLinear(input_size, hidden_sizes[0], m[0:2]),\n",
        "            nn.ReLU()\n",
        "        ]\n",
        "        for i in np.arange(1, len(hidden_sizes)):\n",
        "            layers += [\n",
        "                MaskedLinear(hidden_sizes[i-1], hidden_sizes[i], m[i:i + 2]),\n",
        "                nn.ReLU()\n",
        "            ]\n",
        "        \n",
        "        layers += [\n",
        "            MaskedLinear(hidden_sizes[-1], input_size, (m[-1], last_m))\n",
        "        ]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #x = x.reshape(-1, self.d)\n",
        "        x = self.layers(x)\n",
        "        x = x.reshape(-1, self.d, self.n)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6xHYQynqU8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normal_distr = torch.distributions.normal.Normal()\n",
        "def normal(x, mu, sigma, pi):\n",
        "  batch_size = x.shape[0]\n",
        "  k = mu.shape[0]\n",
        "  x = x.repeat(k).reshape(k, batch_size).transpose(0, 1)\n",
        "  x = (x - mu) / sigma\n",
        "  return (normal_distr(x) * pi).sum(dim=1)\n",
        "\n",
        "def prob(x, mu, sigmam, pi):\n",
        "  batch_size = x.shape[0]\n",
        "  k = mu.shape[0]\n",
        "  x = x.repeat(k).reshape(k, batch_size).transpose(0, 1)\n",
        "  norm = torch.exp(-(x - mu) ** 2 / (s * sigma)) / (2 * np.pi * sigma) ** 0.5\n",
        "  return torch.log(norm * pi).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss5B-mibgP1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Flow(nn.Module):\n",
        "    def __init__(self, k):\n",
        "      super(Flow, self).__init__()\n",
        "      self.pi = nn.Sequential(MADE(2, k, [k, k]), nn.Softmax(dim=-1))\n",
        "      self.sigma = MADE(2, k, [k, k])\n",
        "      self.mu = MADE(2, k, [k, k])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = x[:,0]\n",
        "        x2 = x[:,1]\n",
        "        # (batch_size, 2, k)\n",
        "        pi, sigma, mu = self.pi(x), self.sigma(x), self.mu(x)\n",
        "        sigma = sigma ** 2\n",
        "        pi1, pi2 = pi[:,0,:], pi[:,1,:]\n",
        "        mu1, mu2 = mu[:,0,:], mu[:,1,:]\n",
        "        sigma1, sigma2 = sigma[:,0,:], sigma[:,1,:]\n",
        "        z1 = normal(x1, mu1, sigma1, pi1)\n",
        "        z2 = normal(x2, mu2, sigma2, pi2)\n",
        "        z = torch.cat(z1, z2).reshape(-1, 2)\n",
        "        return z, pi, sigma, mu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqYrAl5qxWha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SuperLoss(nn.Module):\n",
        "        \n",
        "    def forward(self, x, pi, sigma, mu):\n",
        "        batch_size = x.shape[0]\n",
        "        pi1, pi2 = pi[:,0,:], pi[:,1,:]\n",
        "        mu1, mu2 = mu[:,0,:], mu[:,1,:]\n",
        "        sigma1, sigma2 = sigma[:,0,:], sigma[:,1,:]\n",
        "        x1 = x[:,0]\n",
        "        x2 = x[:,1]\n",
        "        p1 = prob(x1, mu1, sigma1, pi1)\n",
        "        p2 = prob(x2, mu2, sigma2, pi2)\n",
        "        return (p1 + p2) / batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDKrad-QQBVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Flow(K).to(device)\n",
        "losser = SuperLoss()\n",
        "dataset_train = torch.utils.data.DataLoader(X_train, batch_size=512, shuffle=True)\n",
        "dataset_test = torch.utils.data.DataLoader(X_test, batch_size=512, shuffle=True)\n",
        "optimizer = torch.optim.Adam(params=model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PBOKHD7Nbdi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "a091601b-e87f-4515-a47a-97806bdccf13"
      },
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "n_epochs = 5\n",
        "for epoch in tqdm(range(n_epochs)):\n",
        "    for X_batch in tqdm(dataset_train):\n",
        "        X_batch = X_batch.to(device)\n",
        "        print(X_batch.shape)\n",
        "        optimizer.zero_grad()\n",
        "        z, pi, sigma, mu = model(X_batch)\n",
        "        loss = losser(X_batch, pi, sigma, mu)\n",
        "        train_loss = loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        with torch.no_grad():\n",
        "          cur_losses = []\n",
        "          for test_batch in dataset_test:\n",
        "            z, pi, sigma, mu = model(X_batch)\n",
        "            loss = losser(test_batch, pi, sigma, mu)\n",
        "            cur_losses.append(test_loss)\n",
        "          test_losses.append(np.mean(cur_losses))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b474135e84dc4a14a1d01f18070a05dd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ed0a407209740b19ab189165f537534",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=157), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 2])\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-e2d888f153db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-e16175c92ad1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# (batch_size, 2, k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-ed1cccb5e763>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#x = x.reshape(-1, self.d)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-ed1cccb5e763>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMADE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (20) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZAqxtaNNhaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_losses(train_losses, test_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdLp8CFGWFjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pm9AsTWV_v-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_sizes=[64, 64]):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = [nn.Linear(1, hidden_sizes[0]), nn.ReLU()]\n",
        "        for i in range(1, len(hidden_sizes)):\n",
        "            layers += [nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]),nn.ReLU()]\n",
        "        layers += [nn.Linear(hidden_sizes[-1], 1)]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-hGIAH0WbMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CouplingLayer(nn.Module):\n",
        "    def __init__(self, hidden_sizes = [64, 64], swap):\n",
        "        super(CouplingLayer, self).__init__()\n",
        "        self.swap = swap\n",
        "        self.s = MLP()\n",
        "        self.t = MLP()\n",
        "\n",
        "        \n",
        "    def forward(self, x1, x2):\n",
        "        if swap:\n",
        "          x1, x2 = x2, x1\n",
        "        y1 = x1\n",
        "        y2 = x2 * torch.exp(self.s(x1)) + self.t(x1)\n",
        "        return y1, y2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH63iD1Da8pV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActNorm, self).__init__()\n",
        "        self.s = nn.Parameter(torch.zeros())\n",
        "        self.t = MLP()\n",
        "        \n",
        "    def forward(self, x1, x2):\n",
        "        y1 = self.s * x1 + self.t\n",
        "        y2 = self.s * x2 + self.t\n",
        "        return y1, y2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYKzMH06ZXq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RealNVP(nn.Module):\n",
        "    def __init__(self, hidden_sizes = [64, 64]):\n",
        "        super(RealNVP, self).__init__()\n",
        "        layers = [nn.Linear(1, hidden_sizes[0]), nn.ReLU()]\n",
        "        for i in range(1, len(hidden_sizes)):\n",
        "            layers += [nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]),nn.ReLU()]\n",
        "        layers += [nn.Linear(hidden_sizes[-1], 1)]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = x[:,0]\n",
        "        x2 = x[:,1]\n",
        "        y1 = x1\n",
        "        y2 = x2 * torch.exp(self.s(x1)) + self.t(x1)\n",
        "        return y1, y2"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}